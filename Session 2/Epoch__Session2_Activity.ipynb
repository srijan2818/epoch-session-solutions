{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOkOxUlLf2y_"
      },
      "source": [
        "# Introduction to Deep Learning  \n",
        "## Building a Neural Network from Scratch\n",
        "\n",
        "In this notebook, we will develop a foundational understanding of neural networks by constructing one from first principles.\n",
        "\n",
        "We will NOT begin with complex libraries or mathematical derivations. Instead, we will focus on the core computational idea behind deep learning.\n",
        "\n",
        "By the end of this notebook, you will understand:\n",
        "\n",
        "• What a neuron is  \n",
        "• How neurons combine to form networks  \n",
        "• What forward propagation means  \n",
        "• How modern frameworks like PyTorch implement these ideas  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngVSwGI1f5FE"
      },
      "source": [
        "# What is an Artificial Neuron?\n",
        "\n",
        "An artificial neuron is a simple mathematical function that performs three steps:\n",
        "\n",
        "### Step 1 — Multiply inputs by weights  \n",
        "Each input is assigned an importance value called a **weight**.\n",
        "\n",
        "### Step 2 — Add the weighted inputs  \n",
        "The neuron computes a weighted sum.\n",
        "\n",
        "### Step 3 — Add a bias  \n",
        "The bias allows the neuron to shift its output.\n",
        "\n",
        "### Mathematical Form:\n",
        "\n",
        "output = (input₁ × weight₁) + (input₂ × weight₂) + bias\n",
        "\n",
        "This computation is known as **forward propagation**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "imem1xZaf8F6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Neuron output: 1.4000000000000004\n"
          ]
        }
      ],
      "source": [
        "# Creating Our First Artificial Neuron\n",
        "\n",
        "# Let us assume we want to predict whether a student will pass an exam\n",
        "# based on two inputs:\n",
        "# 1. Number of hours studied\n",
        "# 2. Number of hours slept\n",
        "\n",
        "\n",
        "# Input variables\n",
        "study_hours = 4\n",
        "sleep_hours = 6\n",
        "\n",
        "# Weight variables\n",
        "# These represent how important each input is\n",
        "weight_study = 0.8\n",
        "weight_sleep = 0.2\n",
        "\n",
        "# Bias variable\n",
        "# This represents the neuron's tendency to output higher or lower values\n",
        "bias = -3\n",
        "\n",
        "\n",
        "# Forward propagation:\n",
        "# Multiply each input by its weight and add the bias\n",
        "output = (study_hours * weight_study) + (sleep_hours * weight_sleep) + bias\n",
        "\n",
        "\n",
        "# Display the result\n",
        "print(\"Neuron output:\", output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uwx8q6FzgJg5"
      },
      "source": [
        "## Experiment\n",
        "\n",
        "Modify the following variables and rerun the cell:\n",
        "\n",
        "• Increase `weight_study`  \n",
        "• Decrease `weight_sleep`  \n",
        "• Change the bias  \n",
        "\n",
        "Observe how the output changes.\n",
        "\n",
        "Reflect on the following:\n",
        "\n",
        "- Which variable appears more important for predicting success?\n",
        "- What happens if the bias becomes very large?\n",
        "- Can you force the neuron to always produce a positive output?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0pm0XVKgTDO"
      },
      "source": [
        "# Activation Functions\n",
        "\n",
        "Currently, the neuron outputs a raw numerical value.\n",
        "\n",
        "However, neural networks typically pass this value through an **activation function**.\n",
        "\n",
        "An activation function introduces **non-linearity**, allowing networks to learn complex patterns.\n",
        "\n",
        "We will use one of the simplest and most widely used activation functions:\n",
        "\n",
        "## ReLU (Rectified Linear Unit)\n",
        "\n",
        "ReLU(x) = max(0, x)\n",
        "\n",
        "If the input is negative, the function returns 0.  \n",
        "Otherwise, it returns the input unchanged.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "PBJcqgq9gUK3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output after ReLU: 1.4000000000000004\n"
          ]
        }
      ],
      "source": [
        "# Defining the ReLU activation function\n",
        "\n",
        "def relu(x):\n",
        "    # max(0, x) returns the greater value between 0 and x\n",
        "    return max(0, x)\n",
        "\n",
        "\n",
        "# Apply activation to the neuron output\n",
        "activated_output = relu(output)\n",
        "\n",
        "print(\"Output after ReLU:\", activated_output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcAg_4CEgdHM"
      },
      "source": [
        "## Experiment\n",
        "\n",
        "Set the bias to a very negative number such as:\n",
        "\n",
        "bias = -10\n",
        "\n",
        "Run the neuron again.\n",
        "\n",
        "Notice that the output becomes 0 after activation.\n",
        "\n",
        "This situation is sometimes informally described as a neuron becoming inactive.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImybUDyIgi1U"
      },
      "source": [
        "# Building a Small Neural Network\n",
        "\n",
        "A single neuron is limited in what it can represent.\n",
        "\n",
        "Neural networks gain their power by combining many neurons into **layers**.\n",
        "\n",
        "We will now construct a small network with:\n",
        "\n",
        "• 2 input features  \n",
        "• 1 hidden layer containing 3 neurons  \n",
        "• 1 output neuron  \n",
        "\n",
        "This structure is called a **fully connected neural network** because every neuron receives all inputs from the previous layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "E5LZRQ6qgldi"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hidden layer outputs (before activation):\n",
            "3.0 -0.20000000000000018 2.2\n"
          ]
        }
      ],
      "source": [
        "# Hidden Layer Computation\n",
        "\n",
        "study = 4\n",
        "sleep = 6\n",
        "\n",
        "\n",
        "# Neuron 1\n",
        "n1 = (study * 0.5) + (sleep * 0.5) - 2\n",
        "\n",
        "# Neuron 2\n",
        "n2 = (study * 1.0) + (sleep * -0.2) - 3\n",
        "\n",
        "# Neuron 3\n",
        "n3 = (study * -0.3) + (sleep * 0.9) - 2\n",
        "\n",
        "\n",
        "print(\"Hidden layer outputs (before activation):\")\n",
        "print(n1, n2, n3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "id5AiA9Rgn72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hidden layer outputs (after activation):\n",
            "3.0 0 2.2\n"
          ]
        }
      ],
      "source": [
        "# Applying ReLU to the hidden layer\n",
        "\n",
        "n1 = relu(n1)\n",
        "n2 = relu(n2)\n",
        "n3 = relu(n3)\n",
        "\n",
        "print(\"Hidden layer outputs (after activation):\")\n",
        "print(n1, n2, n3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhpFKKaWgs0W"
      },
      "source": [
        "Each neuron can learn a different pattern.\n",
        "\n",
        "For example:\n",
        "\n",
        "• One neuron may emphasize studying  \n",
        "• Another may emphasize sleep  \n",
        "• Another may detect imbalance  \n",
        "\n",
        "The network combines these intermediate signals to form a final decision.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "_Iw_rlL5gq_8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final network score: 2.56\n",
            "Prediction: PASS\n"
          ]
        }
      ],
      "source": [
        "# Output Layer\n",
        "\n",
        "final_output = (n1 * 0.6) + (n2 * 0.3) + (n3 * 0.8) - 1\n",
        "\n",
        "print(\"Final network score:\", final_output)\n",
        "\n",
        "\n",
        "# Convert the numerical output into a decision\n",
        "if final_output > 0:\n",
        "    print(\"Prediction: PASS\")\n",
        "else:\n",
        "    print(\"Prediction: FAIL\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5-DFPn4g3jR"
      },
      "source": [
        "## Exploration\n",
        "\n",
        "Adjust the weights in the hidden or output layer and observe how the prediction changes.\n",
        "\n",
        "Consider the following challenges:\n",
        "\n",
        "• Can you make the network always predict PASS?  \n",
        "• Can you make sleep more influential than studying?  \n",
        "• Can you force the network to always predict FAIL?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vg051RyMg6pc"
      },
      "source": [
        "# Implementing the Same Idea Using PyTorch\n",
        "\n",
        "Modern deep learning libraries automate these computations.\n",
        "\n",
        "We will now use **PyTorch**, one of the most widely used deep learning frameworks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "lzCob4RWg9Y9"
      },
      "outputs": [],
      "source": [
        "# Import PyTorch\n",
        "import torch\n",
        "\n",
        "# Import the neural network module\n",
        "import torch.nn as nn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "U8ZO-pZHhAeV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Weights:\n",
            "Parameter containing:\n",
            "tensor([[ 0.4620,  0.0735],\n",
            "        [-0.2121, -0.6142],\n",
            "        [ 0.6514,  0.3762]], requires_grad=True)\n",
            "\n",
            "Bias:\n",
            "Parameter containing:\n",
            "tensor([ 0.6573, -0.0890,  0.4851], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "# Creating a Linear Layer\n",
        "\n",
        "# nn.Linear(input_features, output_features)\n",
        "\n",
        "layer = nn.Linear(2, 3)\n",
        "\n",
        "print(\"Weights:\")\n",
        "print(layer.weight)\n",
        "\n",
        "print(\"\\nBias:\")\n",
        "print(layer.bias)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lm5sStwjhEMV"
      },
      "source": [
        "Observe that PyTorch automatically creates:\n",
        "\n",
        "• A weight matrix  \n",
        "• A bias vector  \n",
        "\n",
        "These are the same quantities we manually defined earlier.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "XlRJ3gbehHU2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer output:\n",
            "tensor([ 2.9462, -4.6226,  5.3479], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# Passing data through the layer\n",
        "\n",
        "inputs = torch.tensor([4.0, 6.0])\n",
        "\n",
        "output = layer(inputs)\n",
        "\n",
        "print(\"Layer output:\")\n",
        "print(output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgLegYHyhXeJ"
      },
      "source": [
        "# Task 1 - Build a Neural Network Using PyTorch\n",
        "\n",
        "You will now construct a small neural network.\n",
        "\n",
        "## Architecture Requirements:\n",
        "\n",
        "Input layer → 2 features  \n",
        "Hidden layer → 4 neurons  \n",
        "Output layer → 1 neuron  \n",
        "\n",
        "Both layers must be fully connected.\n",
        "\n",
        "Use ReLU activation after the hidden layer.\n",
        "\n",
        "---\n",
        "\n",
        "## Your Objectives:\n",
        "\n",
        "1. Create the hidden layer  \n",
        "2. Create the output layer  \n",
        "3. Pass data forward through the network  \n",
        "4. Print the final output  \n",
        "\n",
        "Complete the code below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "vHRO2fUBhXCh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Network output:\n",
            "tensor([0.7066], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "# Create a sample input tensor\n",
        "inputs = torch.tensor([5.0, 3.0])\n",
        "\n",
        "\n",
        "# TODO:\n",
        "# Create a hidden layer with 2 input features and 4 output features\n",
        "hidden_layer = nn.Linear(2, 4)\n",
        "\n",
        "\n",
        "# TODO:\n",
        "# Create an output layer with 4 input features and 1 output feature\n",
        "output_layer = nn.Linear(4, 1)\n",
        "\n",
        "\n",
        "# Forward propagation\n",
        "\n",
        "# Pass inputs through hidden layer\n",
        "hidden_output = hidden_layer(inputs)\n",
        "\n",
        "# Apply ReLU activation\n",
        "hidden_output = torch.relu(hidden_output)\n",
        "\n",
        "# Pass through output layer\n",
        "final_output = output_layer(hidden_output)\n",
        "\n",
        "\n",
        "print(\"Network output:\")\n",
        "print(final_output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiJbVaOAhjyY"
      },
      "source": [
        "# Key Takeaways\n",
        "\n",
        "• Neural networks are compositions of simple mathematical operations.  \n",
        "• Forward propagation is simply weighted addition followed by activation.  \n",
        "• Deep learning frameworks automate these computations efficiently.\n",
        "\n",
        "You now understand the core computational idea behind modern AI systems.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8aj_NToDN_a"
      },
      "source": [
        "# Task 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJ7rnZ9aDQf0"
      },
      "source": [
        "So far, our neural network produces an output.\n",
        "\n",
        "But how do we know if the prediction is good or bad?\n",
        "\n",
        "We need a way to **measure error**.\n",
        "\n",
        "This is the role of a **loss function**.\n",
        "\n",
        "A loss function tells us how far the network's prediction is from the correct answer.\n",
        "\n",
        "Lower loss = better predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zr3ZEg9KDX6i"
      },
      "source": [
        "## Mean Squared Error (MSE)\n",
        "\n",
        "One of the simplest loss functions is **Mean Squared Error**.\n",
        "\n",
        "Steps:\n",
        "\n",
        "1. Compute the difference between prediction and true value  \n",
        "2. Square the difference  \n",
        "3. Take the average  \n",
        "\n",
        "### Formula:\n",
        "\n",
        "MSE = average((prediction − true value)²)\n",
        "\n",
        "Why square the error?\n",
        "\n",
        "• Prevents negative values from cancelling positives  \n",
        "• Penalizes large mistakes more heavily\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUzObic-DZ6I"
      },
      "source": [
        "## Your Task\n",
        "\n",
        "You are given a neural network and some sample students.\n",
        "\n",
        "Each student has:\n",
        "\n",
        "[study hours, sleep hours]\n",
        "\n",
        "The label represents:\n",
        "\n",
        "1 → Pass  \n",
        "0 → Fail  \n",
        "\n",
        "Your objectives:\n",
        "\n",
        "1. Pass the data through the network  \n",
        "2. Compute the loss using Mean Squared Error  \n",
        "3. Print the loss  \n",
        "\n",
        "Observe what the loss value tells you about the network.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "kfeeP8mFma_u"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 0.2200523316860199\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "# Define the network\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(2, 4),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(4, 1)\n",
        ")\n",
        "\n",
        "\n",
        "# Sample dataset\n",
        "students = torch.tensor([\n",
        "    [5.0, 7.0],\n",
        "    [2.0, 3.0],\n",
        "    [6.0, 2.0],\n",
        "    [1.0, 1.0]\n",
        "])\n",
        "\n",
        "labels = torch.tensor([\n",
        "    [1.0],\n",
        "    [0.0],\n",
        "    [1.0],\n",
        "    [0.0]\n",
        "])\n",
        "\n",
        "# TODO: Generate predictions\n",
        "predictions = model(students)\n",
        "probs = torch.sigmoid(predictions)\n",
        "\n",
        "\n",
        "# TODO: Create the loss function\n",
        "loss_fn = lambda probs : torch.mean((labels-probs)**2)\n",
        "\n",
        "# print(*probs) \n",
        "# TODO: Compute the loss\n",
        "loss = loss_fn(probs)\n",
        "\n",
        "# print(predictions)\n",
        "print(\"Loss:\", loss.item())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
